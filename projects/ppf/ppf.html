<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="PPF: Pre-training and Preservative Fine-tuning of Humanoid Locomotion via Model-Assumption-based Regularization">
  <meta name="keywords" content="humanoid, reinforcement learning, control, legged robot, reinforcement learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PPF: Pre-training and Preservative Fine-tuning of Humanoid Locomotion via Model-Assumption-based Regularization</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>
  <!-- add this when related work is done (e.g. digit ifm) -->
<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://faculty.cc.gatech.edu/~sha9/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">PPF: Pre-training and Preservative Fine-tuning of Humanoid Locomotion via Model-Assumption-based Regularization</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://hyunyoungjung.github.io/">Hyunyoung Jung*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://guzhaoyuan.com/">Zhaoyuan Gu*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.me.gatech.edu/faculty/zhao">Ye Zhao</a><sup>1</sup>, 
            </span>
            <span class="author-block">
              <a href="https://www.dynamicrobot.kaist.ac.kr/">Hae-Won Park</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://faculty.cc.gatech.edu/~sha9/">Sehooon Ha</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Georgia Institute of Technology,</span>
            <span class="author-block"><sup>2</sup>Korea Advanced Institute of Science and Technology (KAIST)</span>
            <br>
            <span class="author-block"><sup>*</sup>These two authors contributed equally</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://ieeexplore.ieee.org/abstract/document/10268037"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>RA-L</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2504.09833"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/xwyAuPgVbeA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted playsinline webkit-playsinline loop preload="auto"
        style="width:100%;height:auto;max-height:70vh;display:block"
        poster="data:image/gif;base64,R0lGODlhAQABAAAAACw=">
        <source src="./static/videos/teaser.mp4" type="video/mp4">
      </video>
      <script>
        (function () {
          const v = document.getElementById('teaser');
          if (!v) return;

          v.defaultMuted = true;
          v.muted = true;
          v.autoplay = true;
          v.setAttribute('muted', '');
          v.setAttribute('playsinline', '');
          v.setAttribute('webkit-playsinline', '');

          const tryPlay = () =>
            v.play().catch(() => v.setAttribute('controls', ''));

          // Retry a few times while the layout/buffer settles.
          let attempts = 0;
          const tick = () => {
            if (v.readyState >= 2) {           // HAVE_CURRENT_DATA
              tryPlay();
            } else if (attempts++ < 15) {
              setTimeout(tick, 150);
            } else {
              v.setAttribute('controls', '');  // graceful fallback
            }
          };

          // Kick a fresh load to avoid stale initial state.
          v.load();
          // Fire at multiple readiness milestones (different browsers fire different ones first).
          ['loadedmetadata','loadeddata','canplay','canplaythrough'].forEach(evt =>
            v.addEventListener(evt, tryPlay, { once: true })
          );
          // Also run our small retry loop.
          tick();

          // Last-ditch fallback: first user gesture starts it.
          const onInteract = () => { tryPlay(); window.removeEventListener('pointerdown', onInteract); };
          window.addEventListener('pointerdown', onInteract, { once: true });

          // If the tab was backgrounded at first paint, try again when visible.
          document.addEventListener('visibilitychange', () => {
            if (!document.hidden && v.paused) tryPlay();
          });
        })();
      </script>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Humanoid locomotion is a challenging task due to its inherent complexity and high-dimensional dynamics, as well as the need to adapt to diverse and unpredictable environments. In this work, we introduce a novel learning framework for effectively training a humanoid locomotion policy that imitates the behavior of a model-based controller while extending its capabilities to handle more complex locomotion tasks, such as more challenging terrain and higher velocity commands. Our framework consists of three key components: pre-training through imitation of the model-based controller, fine-tuning via reinforcement learning, and model-assumption-based regularization (MAR) during fine-tuning. In particular, MAR aligns the policy with actions from the model-based controller only in states where the model assumption holds to prevent catastrophic forgetting. We evaluate the proposed framework through comprehensive simulation tests and hardware experiments on a full-size humanoid robot, Digit, demonstrating a forward speed of 1.5 m/s and robust locomotion across diverse terrains, including slippery, sloped, uneven, and sandy terrains.
          </p>

        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>


<!-- <section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
        
        <h2 class="title is-3 has-text-centered">Learning Framework</h2>
        
        <div class="columns is-centered">          
          <img id="learning-framework" src="static/images/method_overview.png" width="80%" 
          style="padding-top: 70px; padding-bottom: 20px;">
        </div>
        <p class="has-text-justified">
          A robot control policy utilizes human motions and sensory information 
          to generate robot actions for interacting with the environment. 
          The Robot2Human-Mapper is then used to reconstruct the input human motion pose from the robot's pose. 
          Lastly, the Human2Robot-Mapper is used to reconstruct the robot pose from the reconstructed human pose. 
          Both mappers are trained via supervised learning using the difference between the real and reconstructed poses.
          These differences are also utilized for constructing a correspondence reward function to train the robot control policy using PPO.
        </p>
      </div>
    </div>
  </div>
</section>     -->
<!-- simulation demo -->
<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
        <h1 class="title is-3">Motion Transfer Demo</h1>
      </div>
    </div>        
    <div class="tabs-widget">
      <div class="tabs is-centered">
        <ul class="is-marginless">
          <li class="is-active"><a>Dancing</a></li>
          <li><a>Hopping</a></li>
          <li><a>Running</a></li>
        </ul>
      </div>
    
      <div class="tabs-content">
        <div row>
          <div class="columns is-vcentered has-text-centered is-full-width">
            <div class="column">
              <h2 class="title is-5">Human Motion</h2>
              <video controls autoplay muted preload loop>
                <source src="static/videos/human_videos/h_motion_9.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column">
              <h2 class="title is-5">Robot Motion</h2>
              <video controls autoplay muted preload loop>
                <source src="static/videos/robot_videos/r_motion_9.mp4" type="video/mp4">
              </video>
            </div>
          </div>

        </div>

        <div row>
          <div class="columns is-vcentered has-text-centered is-full-width">
            <div class="column">
              <h2 class="title is-5" >Human Motion</h2>
              <video controls autoplay muted preload loop>
                <source src="static/videos/human_videos/h_motion_4.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column">
              <h2 class="title is-5">Robot Motion</h2>
              <video controls autoplay muted preload loop>
                <source src="static/videos/robot_videos/motion_4.mp4" type="video/mp4">
              </video>
            </div>
          </div>

        </div>

        <div row>
          <div class="columns is-vcentered has-text-centered is-full-width">
            <div class="column">
              <h2 class="title is-5">Human Motion</h2>
              <video controls autoplay muted preload loop>
                <source src="static/videos/human_videos/h_motion_10.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column">
              <h2 class="title is-5">Robot Motion</h2>
              <video controls autoplay muted preload loop>
                <source src="static/videos/robot_videos/r_motion_10.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>
  
      </div>
    </div>
  </div>
</section> -->
<!-- /simulation demo -->  

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code> Comming Soon!</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website has been modified from <a href="https://camp-nerf.github.io/"> CamP</a> and 
            <a href="https://github.com/nerfies/nerfies.github.io"> Nerfies</a>. <br>
            Last update: Oct.2023
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
